{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from zipfile import is_zipfile, ZipFile\n",
    "from pickle import load\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import array, stack\n",
    "import torch \n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch import nn \n",
    "from torch import optim \n",
    "# from torchsummary import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Unzip precoder dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Directory already exists!\n"
     ]
    }
   ],
   "source": [
    "dir_name = 'datasets'\n",
    "if os.path.exists(dir_name):\n",
    "    print(\"Data has already been unzipped.\")\n",
    "else:\n",
    "    if os.path.exists('datasets.zip'):\n",
    "        if is_zipfile('datasets.zip'):\n",
    "            with ZipFile('datasets.zip', 'r') as zip_data_obj:\n",
    "                zip_data_obj.extractall()\n",
    "        else:\n",
    "            print('File exists, but is not a zip archive.')\n",
    "    else:\n",
    "        print('datasets.zip was not found.')\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load precoder dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(dir_name, fading, num_ant, SNR_dB, bit_codebook):\n",
    "\n",
    "    if not os.path.exists(dir_name):\n",
    "        os.makedirs(dir_name)\n",
    "    file_name = f'{dir_name}/{fading}_precoder_data_{num_ant}_ant_SNR_{SNR_dB}dB_{bit_codebook}_bit_codebk'\n",
    "    print(file_name)\n",
    "    with open(f'{file_name}.pkl', 'rb') as f:\n",
    "        precoders, labels = load(f)\n",
    "    return precoders, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(precoders):\n",
    "    # test plots\n",
    "    for i in range(2):\n",
    "        plt.scatter(precoders[i][0::2], precoders[i][1::2])\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precoder dataset class that packages the dataset into a format compatible with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrecoderDataset(Dataset):\n",
    "    def __init__(self, data, target, transform=None):\n",
    "        self.data = torch.from_numpy(data).float()\n",
    "        self.target = torch.from_numpy(target).long()\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index]\n",
    "        y = self.target[index]\n",
    "        \n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "        return x, y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(Model, self).__init__()\n",
    "        # define layers and activation function as class parameters\n",
    "        self.fc1 = nn.Linear(in_features=input_size, out_features=100)\n",
    "        self.fc2 = nn.Linear(in_features=100, out_features=100)\n",
    "        self.output = nn.Linear(in_features=100, out_features=output_size)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.softmax = torch.nn.Softmax()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # define NN connections here, output of last layer is pred\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        pred = self.softmax(self.output(x))\n",
    "        return pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Essential System Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iter = 100000\n",
    "fading = 1\n",
    "num_ant = [2, 4]\n",
    "SNR_dB = [10, 20, 45]\n",
    "bit_codebook = [1, 2]\n",
    "visual = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "num_epochs = 100 # Number of times you go through the whole dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Simulation iterations\n",
    "1. Load dataset\n",
    "2. Visualize, if necessary \n",
    "3. Create instance of precoder dataset class (Package dataset such that it is compatible for PyTorch)\n",
    "4. Split dataset into 80% training and 20% validation\n",
    "5. Create PyTorch dataloaders for training and validation data (Dataloaders divide the dataset into chuncks called batches. We can pass data to the neural network one batch at a time) \n",
    "6. Create an instance of neural network class\n",
    "7. Define the loss and optimizer\n",
    "8. Epoch - one pass of the entire dataset (training + validation)\n",
    "9. In each epoch you iterate over several batches (Batch - chunk of data that is fed to the neural network in one go) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for SNR in SNR_dB:\n",
    "    for ant in num_ant:\n",
    "        for bit in bit_codebook:\n",
    "            print(f'======{SNR} dB, {ant} antennas, {bit} bit codebook======')\n",
    "            \n",
    "            precoders, labels = load_dataset('datasets', fading, ant, SNR, bit)\n",
    "            if visual == 1:\n",
    "                visualize(precoders)\n",
    "\n",
    "            precoder_data = stack(precoders, axis=0)\n",
    "            precoder_labels = array(labels)    \n",
    "\n",
    "            precoder_dataset = PrecoderDataset(precoder_data, precoder_labels)\n",
    "            train, val = random_split(precoder_dataset, [int(0.8*max_iter), int(0.2*max_iter)])\n",
    "            train_loader = DataLoader(train, batch_size=128, shuffle=True, num_workers=0)\n",
    "            val_loader = DataLoader(val, batch_size=128, shuffle=True, num_workers=0)\n",
    "            \n",
    "            num_classes = 2**bit\n",
    "            nn_input_size = 2*ant*ant\n",
    "            nn_output_size = num_classes\n",
    "            \n",
    "            model = Model(nn_input_size, nn_output_size)\n",
    "\n",
    "            if torch.cuda.is_available():\n",
    "                model.cuda()\n",
    "\n",
    "            optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "            # optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "            loss = nn.CrossEntropyLoss()\n",
    "            for epoch in range(num_epochs):\n",
    "\n",
    "                # TRAINING\n",
    "                model.train()\n",
    "                training_total_correct = 0\n",
    "                training_losses = list()\n",
    "                for batch in train_loader:\n",
    "\n",
    "                    x, y = batch # extracted from the batch \n",
    "\n",
    "                    if torch.cuda.is_available():\n",
    "                        x = x.cuda()\n",
    "                        y = y.cuda()\n",
    "\n",
    "                    # step 1: forward pass \n",
    "                    prob = model(x) \n",
    "\n",
    "                    output = prob\n",
    "                    target = y\n",
    "\n",
    "                    # step 2: compute objective function - measuring distance between the output of the network vs actual answer \n",
    "                    obj_func = loss(output, target)\n",
    "\n",
    "                    # step 3: clear the gradients \n",
    "                    model.zero_grad()\n",
    "\n",
    "                    # step 4: accumulate partial derivatives of obj_func wrt parameters \n",
    "                    obj_func.backward()\n",
    "\n",
    "                    # step 5: step in the opposite direction of the gradient \n",
    "                    optimizer.step()\n",
    "\n",
    "                    training_losses.append(obj_func.item())\n",
    "\n",
    "                    max_prob, max_ind = torch.max(output, 1)\n",
    "                    for ind in range(len(max_ind)):\n",
    "                        if torch.equal(max_ind[ind], y[ind]):\n",
    "                            training_total_correct += 1    \n",
    "\n",
    "                # VALIDATION\n",
    "                model.eval()\n",
    "\n",
    "                validation_total_correct = 0\n",
    "                validation_losses = list()\n",
    "                for batch in val_loader:\n",
    "\n",
    "                    x, y = batch # extracted from the batch \n",
    "\n",
    "                    if torch.cuda.is_available():\n",
    "                        x = x.cuda()\n",
    "                        y = y.cuda()\n",
    "\n",
    "                    # step 1: forward pass \n",
    "                    with torch.no_grad():\n",
    "                        prob = model(x) \n",
    "\n",
    "                    output = prob\n",
    "                    target = y\n",
    "\n",
    "                    # step 2: compute objective function - measuring distance between the output of the network vs actual answer \n",
    "                    obj_func = loss(output, target)\n",
    "                    validation_losses.append(obj_func.item())\n",
    "\n",
    "                    max_prob, max_ind = torch.max(output, 1)\n",
    "\n",
    "                    for ind in range(len(max_ind)):\n",
    "                        if torch.equal(max_ind[ind], y[ind]):\n",
    "                            validation_total_correct += 1\n",
    "                training_accuracy = (training_total_correct/len(train))*100  \n",
    "                validation_accuracy = (validation_total_correct/len(val))*100   \n",
    "\n",
    "                print(f'Epoch {epoch + 1}, training loss: {torch.tensor(training_losses).mean():.8f}, training accuracy: {training_accuracy}%, validation loss: {torch.tensor(validation_losses).mean():.8f}, validation accuracy: {validation_accuracy}%')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======10 dB, 2 antennas, 1 bit codebook======\n",
      "datasets/1_precoder_data_2_ant_SNR_10dB_1_bit_codebk\n",
      "Epoch 1, training loss: 0.59856826, training accuracy: 89.3675%, validation loss: 0.47822833, validation accuracy: 93.315%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bup48\\anaconda3\\envs\\Py37\\lib\\site-packages\\ipykernel_launcher.py:15: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-24-9b76d5bd2df9>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     52\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     53\u001B[0m                     \u001B[1;31m# step 3: clear the gradients\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 54\u001B[1;33m                     \u001B[0mmodel\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mzero_grad\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     55\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     56\u001B[0m                     \u001B[1;31m# step 4: accumulate partial derivatives of obj_func wrt parameters\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\Py37\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36mzero_grad\u001B[1;34m(self, set_to_none)\u001B[0m\n\u001B[0;32m   1338\u001B[0m                     \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1339\u001B[0m                         \u001B[0mp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mgrad\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mrequires_grad_\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;32mFalse\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1340\u001B[1;33m                     \u001B[0mp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mgrad\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mzero_\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1341\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1342\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mshare_memory\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mT\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m->\u001B[0m \u001B[0mT\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "for SNR in SNR_dB:\n",
    "    for ant in num_ant:\n",
    "        for bit in bit_codebook:\n",
    "            print(f'======{SNR} dB, {ant} antennas, {bit} bit codebook======')\n",
    "            \n",
    "            precoders, labels = load_dataset('datasets', fading, ant, SNR, bit)\n",
    "            if visual == 1:\n",
    "                visualize(precoders)\n",
    "\n",
    "            precoder_data = stack(precoders, axis=0)\n",
    "            precoder_labels = array(labels)    \n",
    "\n",
    "            precoder_dataset = PrecoderDataset(precoder_data, precoder_labels)\n",
    "            train, val = random_split(precoder_dataset, [int(0.8*max_iter), int(0.2*max_iter)])\n",
    "            train_loader = DataLoader(train, batch_size=128, shuffle=True, num_workers=0)\n",
    "            val_loader = DataLoader(val, batch_size=128, shuffle=True, num_workers=0)\n",
    "            \n",
    "            num_classes = 2**bit\n",
    "            nn_input_size = 2*ant*ant\n",
    "            nn_output_size = num_classes\n",
    "            \n",
    "            model = Model(nn_input_size, nn_output_size)\n",
    "\n",
    "            if torch.cuda.is_available():\n",
    "                model.cuda()\n",
    "\n",
    "            optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "            # optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "            loss = nn.CrossEntropyLoss()\n",
    "            for epoch in range(num_epochs):\n",
    "\n",
    "                # TRAINING\n",
    "                model.train()\n",
    "                training_total_correct = 0\n",
    "                training_losses = list()\n",
    "                for batch in train_loader:\n",
    "\n",
    "                    x, y = batch # extracted from the batch \n",
    "\n",
    "                    if torch.cuda.is_available():\n",
    "                        x = x.cuda()\n",
    "                        y = y.cuda()\n",
    "\n",
    "                    # step 1: forward pass \n",
    "                    prob = model(x) \n",
    "\n",
    "                    output = prob\n",
    "                    target = y\n",
    "\n",
    "                    # step 2: compute objective function - measuring distance between the output of the network vs actual answer \n",
    "                    obj_func = loss(output, target)\n",
    "\n",
    "                    # step 3: clear the gradients \n",
    "                    model.zero_grad()\n",
    "\n",
    "                    # step 4: accumulate partial derivatives of obj_func wrt parameters \n",
    "                    obj_func.backward()\n",
    "\n",
    "                    # step 5: step in the opposite direction of the gradient \n",
    "                    optimizer.step()\n",
    "\n",
    "                    training_losses.append(obj_func.item())\n",
    "\n",
    "                    max_prob, max_ind = torch.max(output, 1)\n",
    "                    for ind in range(len(max_ind)):\n",
    "                        if torch.equal(max_ind[ind], y[ind]):\n",
    "                            training_total_correct += 1    \n",
    "\n",
    "                # VALIDATION\n",
    "                model.eval()\n",
    "\n",
    "                validation_total_correct = 0\n",
    "                validation_losses = list()\n",
    "                for batch in val_loader:\n",
    "\n",
    "                    x, y = batch # extracted from the batch \n",
    "\n",
    "                    if torch.cuda.is_available():\n",
    "                        x = x.cuda()\n",
    "                        y = y.cuda()\n",
    "\n",
    "                    # step 1: forward pass \n",
    "                    with torch.no_grad():\n",
    "                        prob = model(x) \n",
    "\n",
    "                    output = prob\n",
    "                    target = y\n",
    "\n",
    "                    # step 2: compute objective function - measuring distance between the output of the network vs actual answer \n",
    "                    obj_func = loss(output, target)\n",
    "                    validation_losses.append(obj_func.item())\n",
    "\n",
    "                    max_prob, max_ind = torch.max(output, 1)\n",
    "\n",
    "                    for ind in range(len(max_ind)):\n",
    "                        if torch.equal(max_ind[ind], y[ind]):\n",
    "                            validation_total_correct += 1\n",
    "                training_accuracy = (training_total_correct/len(train))*100  \n",
    "                validation_accuracy = (validation_total_correct/len(val))*100   \n",
    "\n",
    "                print(f'Epoch {epoch + 1}, training loss: {torch.tensor(training_losses).mean():.8f}, training accuracy: {training_accuracy}%, validation loss: {torch.tensor(validation_losses).mean():.8f}, validation accuracy: {validation_accuracy}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}